{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INF 510 FINAL PROJECT\n",
    "##### Name: Peter Argo\n",
    "##### submit: 12/13/19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. The names of team member(s)**\n",
    "\n",
    "Peter Argo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. How to run your code (what command-line switches they are, what happens when you invoke the code, etc.)**\n",
    "\n",
    "First clone the repo in the GitHub repository. Execute each cell in this notebook. It is structured in 5 major section that are clearly labeled within the notebook. The MLB_function.py file contains all the required functions and is imported on the first step of the project.  Two .csv files stored in subdirectory “data” are also required.  Additionally, the following packages are needed:\n",
    "\n",
    "- pandas, numpy, os, requests, re, collections, csv, sklearn, pprint, time, and beautifulsoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Any major “gotchas” to the code (i.e. things that don’t work, go slowly, could be improved, etc.)**\n",
    "\n",
    "Surprisingly, I could not find many MLB API’s that provided useful information within the scope of my project. I believe I creatively exercised the utility of the MLB API contained in my project however in order to scrape for all the data, I takes much longer to process then is ideal.  See the \"read this\" section below for tips on processing this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Anything else you feel is relevant to the grading of your project your project.** \n",
    "\n",
    "It was a challenge to make the code robust and efficient at the area of the API data scraping because o fhow much time it too to get the data. I tried to make it as efficient as possible by limiting the number of inputs and lines processed but it does have to pull quite a bit of data and process it to get anything relevant. With time I would optimize the algorithm to reduce the amount of time it takes to process the API data.  The API data redundantly pulls data which is done because of how it iterated through the input team data structure. This could be mitigated with more time  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. What did you set out to study? (i.e. what was the point of your project? This should be close to your Milestone 1 assignment, but if you switched gears or changed things, note it here.)** \n",
    "\n",
    "The original intent was to evaluate which MLB team regular season stats had the greatest impact on predicting the teams success in the playoffs. Originally, I wanted to do some method of multi-class evaluation where each team was assigned a score for how far they advanced in the playoffs. However, I slightly deviated from this premise to look at each postseason series individually and assign a binary classification of “win” or “not win” in order to analyze the team stats data. The project slightly evolved as I gained greater understanding of how to analyze everything and ultimateky did a logistic regression to determine the win probability the team with the better record has of wining their postseason series given the selected regular season features data. \n",
    "\n",
    "I also pivoted from one of my data sources detailed in milestone 1 because earlier on I ran into trouble extracting the data from the initially proposed webpage and due to time constraints, had to move on.  Only after making significant progress on the new data source (fox sports) did I finally discover what my issue was with my old data source. At this point it was too late to go back. The content was largely the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. What did you Discover/what were your conclusions (i.e. what were your findings? Were your original assumptions confirmed, etc.?)**\n",
    "\n",
    "I believe the results were mostly in line with what was expected.  I suspected that if I were to successfully be able to complete this project that I would perhaps get some quasi-favorable results, however given the time constraint and limitations with the data size, no real conclusions should be deduced.  I was under no delusion that I would be able to forecast games with any statistically significant reliability.  However, I was hoping to demonstrate the logic behind this process and hopefully have a result to discuss at the end.\n",
    "\n",
    "Ultimately, I got a result that was better than expected but it certainly fluctuated with how I partition my test set and training set. 20% - 30% test size seemed reasonable for this project. I was fortunate that this yielded a positive model. Bottom line: I now expect to win big money betting on MLB postseason series! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. What difficulties did you have in completing the project?**\n",
    "\n",
    "Scraping the data and formatting the data in a logical manner to be able to manipulate efficiently took about 80% of my time and attention. This was challenging but about as much as anticipated.  The machine learning took less time and was less coding intensive, but it was more esoteric. From a macro perspective, I knew how the statistical analysis should be performed. But when executing the analysis, I had difficulty understanding what the target value should be and how to set up the features for each team in each postseason series.  I came up with a few different approaches after much deliberating but felt that none of them were as comprehensive and insightful as I had hoped. I wanted to make the target value to be a binary classification of “home team win” vs. “home team not win” but I could not get this from my data sources. This would present a clean way to analyze the data and provide a logical paradigm to compare results (assuming the home team wins slightly more than the road team).  I also had difficulties in setting up the features because I had features for two teams and a classifier for the series.  Ultimately, I reasoned to take the difference of the two values, creating a statistical profile for the series itself. Lastly, my binary classifier was “team with better record win” vs. “team with better record not win”. This seemed to be a reasonable choice but I primarily chose this because it was scrapable from my data sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. What skills did you wish you had while you were doing the project?**\n",
    "\n",
    "I wish I was sounder in my statistics background. I was disappointed in the challenges I was running into when completing this analysis. I understood the machine learning topics in a meta perspective but the execution left a lot to be desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. What would you do “next” to expand or augment the project?**\n",
    "\n",
    "The obvious improvement would be to play around with the statistics that I chose and see which permutation of team stats yielded the best results for my analysis. It reasons that simply, more data would be better but some baseball statistics have a lot of overlapping information (batting avg vs. on base percentage for example).  I would also expand the types of analyses beyond just logistic regression and determine which method yielded the best results. Lastly, I would most need to expand my training and test set. Baseball is fundamentally the most probabilistic of the major American sports and thus would likely require the most data to reduce the noise. It might be advantageous to look at every game played individually instead of the entire postseason series as one datapoint. This would require many more data sources. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview:\n",
    "<br>\n",
    "The goal is to evaluate which MLB regular season team statistics predicted success of teams in the postseason\n",
    "<br>\n",
    "This notebook shall scrape data from the following 3 sources:\n",
    "<br>\n",
    "1. Baseball reference\n",
    "<br>\n",
    "2. Fox Sports\n",
    "<br>\n",
    "3. MLB API\n",
    "<br>\n",
    "This notebook shall format the data using SQL and Pandas as primary methods\n",
    "<br>\n",
    "This Notbook shall analyze the data using a logistic regression to draw conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import all modules used to do our analysis. Importing MLB_functions initializes the function contained within and creates our database and tables used to store data. \n",
    "<br>\n",
    "Important note: If you want to re-initialize the tables, you must restart the kernal and import the MLB_functions again as this is where the database is orginally generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(\"C:\\\\Users\\\\Peter Argo\\\\Desktop\\\\INF 510\\\\Project\\\\src\"))\n",
    "from MLB_functions import *\n",
    "import MLB_functions\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import collections\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score , confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "# from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Extract post season series from Baseball Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function baseball_ref_scraper gets the data from baseball reference\n",
    "the data are arrays containing the year the winner, the loser, and the title of each series as well as a booloen \n",
    "determining if the team with more reg season wins won and a cnt value of times a series was played by team with \n",
    "the same number of regular season wins.\n",
    "The value input into baseball_ref_scraper determines the number of post season series that we want to analyse\n",
    "value must be less or equal to than 190"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***READ THIS***\n",
    "<br>\n",
    "This will be more clear as you walk through the notebook, however I reccomend inputing 190 for the number_of_series (maximum possible value.  This will show its robustness. Then execute every cell until you get to Section 3 (API data  extract section). Skip Section 3. Then run all cells in section 4 (Analysis 1 of 2). This will provide the logic used throughout using the largest dataset possible.\n",
    "<br>\n",
    "<br>\n",
    "Subsequently, restart the kernal. Import the above module to reintialize our database. Then Run through the entire notbook as layed out however I would not input a number greater than 25 for number_of_series. The API scraper requires much processing and thus larger numbers take significantly more time to scrape for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input a value below 191:30\n",
      "you chose: 30\n"
     ]
    }
   ],
   "source": [
    "number_of_series = int(input(\"input a value below 191:\"))\n",
    "while number_of_series > 190:\n",
    "    number_of_series = int(input(\"please provide a value lower than 191. Thanks!:\"))\n",
    "print(f\"you chose: {number_of_series}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nationals', 'Astros', 'Nationals', 'Astros', 'Yankees', 'Nationals', 'Cardinals', 'Rays', 'Nationals', 'Red Sox', 'Red Sox', 'Dodgers', 'Red Sox', 'Astros', 'Brewers', 'Dodgers', 'Yankees', 'Rockies', 'Astros', 'Astros', 'Dodgers', 'Yankees', 'Astros', 'Dodgers', 'Cubs', 'Yankees', 'Diamondbacks', 'Cubs', 'Indians', 'Cubs']\n",
      "\n",
      "\n",
      "['Astros', 'Yankees', 'Cardinals', 'Rays', 'Twins', 'Dodgers', 'Braves', 'Athletics', 'Brewers', 'Dodgers', 'Astros', 'Brewers', 'Yankees', 'Indians', 'Rockies', 'Braves', 'Athletics', 'Cubs', 'Dodgers', 'Yankees', 'Cubs', 'Indians', 'Red Sox', 'Diamondbacks', 'Nationals', 'Twins', 'Rockies', 'Indians', 'Blue Jays', 'Dodgers']\n",
      "\n",
      "\n",
      "['2019 World Series', '2019 ALCS', '2019 NLCS', '2019 ALDS1', '2019 ALDS2', '2019 NLDS1', '2019 NLDS2', '2019 ALWC', '2019 NLWC', '2018 World Series', '2018 ALCS', '2018 NLCS', '2018 ALDS1', '2018 ALDS2', '2018 NLDS1', '2018 NLDS2', '2018 ALWC', '2018 NLWC', '2017 World Series', '2017 ALCS', '2017 NLCS', '2017 ALDS1', '2017 ALDS2', '2017 NLDS1', '2017 NLDS2', '2017 ALWC', '2017 NLWC', '2016 World Series', '2016 ALCS', '2016 NLCS']\n",
      "\n",
      "\n",
      "[2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2016, 2016, 2016]\n",
      "\n",
      "\n",
      "[1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0]\n",
      "\n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "(year, winning_team_array, losing_team_array, postseason_series, more_wins, cnt) = baseball_ref_scraper(number_of_series)\n",
    "\n",
    "# print valuse to visualize output if needed\n",
    "\n",
    "print(winning_team_array)\n",
    "print('\\n')\n",
    "print(losing_team_array)\n",
    "print('\\n')\n",
    "print(postseason_series)\n",
    "print('\\n')\n",
    "print(year)\n",
    "print('\\n')\n",
    "print(more_wins)\n",
    "print('\\n')\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of times the team with the winning record wins their postseason series is determined to be approx 50% if the time. The number of times there were equal wins between the competing times was not considered in this calculation. Therefore if you be bet squarely on number of wins a team has vs the opposing team, you could expect to win approximatly half the time. Using wins as our target, we want to improve on these odds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the percent of series where the team with the better record advanced is 46.666666666666664% for this sample\n"
     ]
    }
   ],
   "source": [
    "counter=collections.Counter(more_wins)\n",
    "print(f\"the percent of series where the team with the better record advanced is {counter[1]/(counter[0]+counter[1]-cnt)*100}% for this sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Primary Key database\n",
    "<br>\n",
    "Requires input .csv file from Data subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'Arizona', 'Diamondbacks', 109), (2, 'Atlanta', 'Braves', 130), (3, 'Baltimore', 'Orioles', 110), (4, 'Boston', 'Red Sox', 111), (5, 'Chicago_', 'Cubs', 112), (6, 'Chicago', 'White Sox', 145), (7, 'Cincinnati', 'Reds', 113), (8, 'Cleveland', 'Indians', 114), (9, 'Colorado', 'Rockies', 115), (10, 'Detroit', 'Tigers', 116), (11, 'Houston', 'Astros', 117), (12, 'Kansas City', 'Royals', 118), (13, 'Los Angeles', 'Angels', 108), (14, 'Los Angeles_', 'Dodgers', 119), (15, 'Miami', 'Marlins', 146), (16, 'Milwaukee', 'Brewers', 158), (17, 'Minnesota', 'Twins', 142), (18, 'New York', 'Mets', 121), (19, 'New York_', 'Yankees', 147), (20, 'Oakland', 'Athletics', 133), (21, 'Philadelphia', 'Phillies', 143), (22, 'Pittsburgh', 'Pirates', 134), (23, 'San Diego', 'Padres', 135), (24, 'San Francisco', 'Giants', 137), (25, 'Seattle', 'Mariners', 136), (26, 'St. Louis', 'Cardinals', 138), (27, 'Tampa Bay', 'Rays', 139), (28, 'Texas', 'Rangers', 140), (29, 'Toronto', 'Blue Jays', 141), (30, 'Washington', 'Nationals', 120)]\n"
     ]
    }
   ],
   "source": [
    "val_prim = []\n",
    "with open('..\\data\\MLB teams.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for line in csv_reader:\n",
    "        line[0] = int(line[0])\n",
    "        line[3] = int(line[3])\n",
    "        val_prim.append((line[0],str(line[1]),str(line[2]),line[3]))\n",
    "\n",
    "sql_prim = 'INSERT INTO prim_ID (team_id, team_city, team_name, API_ID) VALUES (?, ?, ?, ?)'\n",
    "\n",
    "cur.executemany(sql_prim,val_prim)\n",
    "\n",
    "cur.execute('SELECT * FROM prim_ID')\n",
    "results = cur.fetchall()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creater Database for each postseason series winning team and losing team\n",
    "(series, winning team, losing team, year, winning team won Boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2019 World Series', 30, 11, 2019, 1), ('2019 ALCS', 11, 19, 2019, 1), ('2019 NLCS', 30, 26, 2019, 1), ('2019 ALDS1', 11, 27, 2019, 0), ('2019 ALDS2', 19, 17, 2019, 1), ('2019 NLDS1', 30, 14, 2019, 1), ('2019 NLDS2', 26, 2, 2019, 0), ('2019 ALWC', 27, 20, 2019, 0), ('2019 NLWC', 30, 16, 2019, 1), ('2018 World Series', 4, 14, 2018, 0), ('2018 ALCS', 4, 11, 2018, 1), ('2018 NLCS', 14, 16, 2018, 0), ('2018 ALDS1', 4, 19, 2018, 1), ('2018 ALDS2', 11, 8, 2018, 0), ('2018 NLDS1', 16, 9, 2018, 1), ('2018 NLDS2', 14, 2, 2018, 1), ('2018 ALWC', 19, 20, 2018, 0), ('2018 NLWC', 9, 5, 2018, 0), ('2017 World Series', 11, 14, 2017, 0), ('2017 ALCS', 11, 19, 2017, 0), ('2017 NLCS', 14, 5, 2017, 0), ('2017 ALDS1', 19, 8, 2017, 1), ('2017 ALDS2', 11, 4, 2017, 0), ('2017 NLDS1', 14, 1, 2017, 0), ('2017 NLDS2', 5, 30, 2017, 0), ('2017 ALWC', 19, 17, 2017, 1), ('2017 NLWC', 1, 9, 2017, 1), ('2016 World Series', 5, 8, 2016, 0), ('2016 ALCS', 8, 29, 2016, 1), ('2016 NLCS', 5, 14, 2016, 0)]\n"
     ]
    }
   ],
   "source": [
    "sql_series = 'INSERT INTO Series (postseason_series, winning_team, losing_team, year, WTW) VALUES (?, ?, ?, ?, ?)'\n",
    "val_series = []\n",
    "counter = 0\n",
    "for i in range(len(winning_team_array)):\n",
    "    val_series.append((postseason_series[i], MLB_functions.read_from_db(winning_team_array[i]), MLB_functions.read_from_db(losing_team_array[i]),year[i],more_wins[i]))\n",
    "    counter += 1\n",
    "cur.executemany(sql_series,val_series)\n",
    "\n",
    "cur.execute('SELECT * FROM Series')\n",
    "results = cur.fetchall()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Extract team statistics from Fox Sports\n",
    "##### Collect hitting and pitching stats for each team. Web scrape FOX sports page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mine data from fox sports. Store the data in the team_stats database, stats table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request pitching data from fox sports\n",
    "my_years = []\n",
    "my_yr = 2019 # enter the current year or any year that you want to start collecting data from\n",
    "while my_yr >= min(year):\n",
    "    my_years.append(my_yr)\n",
    "    my_yr -= 1\n",
    "\n",
    "for the_year in my_years:\n",
    "\n",
    "    r = requests.get(f\"https://www.foxsports.com/mlb/team-stats?season={the_year}&category=PITCHING+RATIOS&group=1&time=0\")\n",
    "    soup = BeautifulSoup(r.content, 'lxml')\n",
    "\n",
    "    # request batting data from fox sports\n",
    "    r_2 = requests.get(f\"https://www.foxsports.com/mlb/team-stats?season={the_year}&category=BATTING&group=1&time=0\")\n",
    "    soup_2 = BeautifulSoup(r_2.content, 'lxml')\n",
    "\n",
    "    main_div_pitching = soup.findAll('table')[0].findAll('tbody')[0]#.findAll('tr')[1].findAll('td')[0].findAll('span')[1]\n",
    "    main_table_pitching = soup.findAll('table')[0].findAll('tbody')[0].findAll('tr')\n",
    "\n",
    "    main_table_batting = soup_2.findAll('table')[0].findAll('tbody')[0].findAll('tr')#[1].findAll('td')[0]\n",
    "    \n",
    "    \n",
    "    team_OBP = []\n",
    "    team_SLG = []\n",
    "    team_Name = []\n",
    "    team_HR = []\n",
    "    for team in main_table_batting:\n",
    "        Name = str(team.findAll('td')[0].findAll('span')[1]).split('>')[1].split('<')[0]\n",
    "        if Name not in team_Name:\n",
    "            team_Name.append(Name)\n",
    "        else:\n",
    "            team_Name.append(Name+\"_\")\n",
    "        team_OBP.append(str(team.findAll('td')[15]).split('>')[1].split('<')[0])\n",
    "        team_SLG.append(str(team.findAll('td')[16]).split('>')[1].split('<')[0])\n",
    "        team_HR.append(str(team.findAll('td')[8]).split('>')[1].split('<')[0])\n",
    "    \n",
    "    team_ERA = []\n",
    "    team_WHIP = []\n",
    "    team_SO9 = []\n",
    "    for team in main_table_pitching:\n",
    "        team_ERA.append(str(team.findAll('td')[17]).split('>')[1].split('<')[0])\n",
    "        team_WHIP.append(str(team.findAll('td')[8]).split('>')[1].split('<')[0])\n",
    "        team_SO9.append(str(team.findAll('td')[12]).split('>')[1].split('<')[0])\n",
    "            \n",
    "    sql_stats = 'INSERT INTO Stats (team_id, year, OBP, SLG, HR, WHIP, ERA, SO9) VALUES (?, ?, ?, ?, ?, ?, ?, ?)'\n",
    "    val_stats = []\n",
    "  \n",
    "    for i in range(len(team_ERA)):\n",
    "        val_stats.append((read_from_db(team_Name[i]),the_year,float(team_OBP[i]), float(team_SLG[i]),float(team_HR[i]),float(team_WHIP[i]),float(team_ERA[i]),float(team_SO9[i])))\n",
    "\n",
    "    cur.executemany(sql_stats,val_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the results if visualization is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 2019, 0.352, 0.495, 288.0, 1.49, 5.56, 7.9)\n",
      "(17, 2019, 0.338, 0.494, 307.0, 1.46, 5.24, 8.6)\n",
      "(4, 2019, 0.34, 0.466, 245.0, 1.46, 5.6, 7.8)\n",
      "(18, 2019, 0.339, 0.49, 306.0, 1.48, 5.2, 7.8)\n",
      "(30, 2019, 0.342, 0.454, 231.0, 1.46, 5.06, 8.6)\n",
      "(9, 2019, 0.326, 0.456, 224.0, 1.45, 5.18, 9.0)\n",
      "(22, 2019, 0.321, 0.42, 163.0, 1.38, 4.99, 7.7)\n",
      "(6, 2019, 0.314, 0.414, 182.0, 1.3, 4.18, 9.0)\n",
      "(2, 2019, 0.336, 0.452, 249.0, 1.37, 4.53, 8.6)\n",
      "(19, 2019, 0.328, 0.442, 242.0, 1.43, 4.79, 8.3)\n",
      "(13, 2019, 0.338, 0.472, 279.0, 1.43, 4.9, 8.4)\n",
      "(27, 2019, 0.325, 0.431, 217.0, 1.38, 4.7, 10.0)\n",
      "(1, 2019, 0.323, 0.434, 220.0, 1.36, 4.19, 8.6)\n",
      "(5, 2019, 0.331, 0.452, 256.0, 1.38, 5.12, 8.8)\n",
      "(8, 2019, 0.323, 0.432, 223.0, 1.3, 4.24, 9.4)\n",
      "(20, 2019, 0.327, 0.448, 257.0, 1.31, 4.25, 8.8)\n",
      "(28, 2019, 0.319, 0.431, 223.0, 1.3, 4.38, 8.4)\n",
      "(12, 2019, 0.309, 0.401, 162.0, 1.3, 4.6, 9.3)\n",
      "(14, 2019, 0.324, 0.422, 220.0, 1.32, 4.1, 9.0)\n",
      "(3, 2019, 0.31, 0.415, 213.0, 1.3, 4.31, 9.6)\n",
      "(21, 2019, 0.319, 0.427, 215.0, 1.33, 4.4, 9.2)\n",
      "(16, 2019, 0.329, 0.438, 250.0, 1.24, 3.97, 8.0)\n",
      "(26, 2019, 0.322, 0.415, 210.0, 1.29, 4.27, 9.5)\n",
      "(7, 2019, 0.315, 0.422, 227.0, 1.35, 4.74, 8.6)\n",
      "(15, 2019, 0.298, 0.375, 146.0, 1.22, 3.76, 9.4)\n",
      "(10, 2019, 0.294, 0.388, 149.0, 1.27, 3.8, 8.7)\n",
      "(24, 2019, 0.302, 0.392, 167.0, 1.17, 3.65, 9.9)\n",
      "(23, 2019, 0.308, 0.41, 219.0, 1.26, 4.18, 9.7)\n",
      "(25, 2019, 0.316, 0.424, 239.0, 1.13, 3.66, 10.3)\n",
      "(29, 2019, 0.305, 0.428, 247.0, 1.1, 3.37, 9.5)\n",
      "(4, 2018, 0.339, 0.453, 208.0, 1.5, 5.19, 7.6)\n",
      "(8, 2018, 0.332, 0.434, 216.0, 1.46, 4.95, 7.3)\n",
      "(27, 2018, 0.333, 0.406, 150.0, 1.4, 4.93, 7.0)\n",
      "(6, 2018, 0.333, 0.41, 167.0, 1.4, 4.65, 7.9)\n",
      "(2, 2018, 0.324, 0.417, 175.0, 1.41, 4.85, 8.1)\n",
      "(9, 2018, 0.322, 0.435, 210.0, 1.34, 4.41, 8.6)\n",
      "(11, 2018, 0.329, 0.425, 205.0, 1.38, 4.5, 8.6)\n",
      "(25, 2018, 0.314, 0.408, 176.0, 1.34, 4.6, 7.7)\n",
      "(30, 2018, 0.335, 0.419, 191.0, 1.43, 4.85, 7.9)\n",
      "(22, 2018, 0.317, 0.407, 157.0, 1.24, 4.13, 8.3)\n",
      "(7, 2018, 0.328, 0.401, 172.0, 1.38, 4.76, 7.8)\n",
      "(20, 2018, 0.325, 0.439, 227.0, 1.31, 3.95, 7.8)\n",
      "(16, 2018, 0.323, 0.424, 218.0, 1.31, 4.0, 8.4)\n",
      "(17, 2018, 0.318, 0.405, 166.0, 1.31, 4.33, 8.7)\n",
      "(13, 2018, 0.333, 0.442, 235.0, 1.29, 4.15, 9.1)\n",
      "(18, 2018, 0.329, 0.451, 267.0, 1.27, 4.07, 8.9)\n",
      "(26, 2018, 0.321, 0.409, 205.0, 1.34, 3.85, 8.3)\n",
      "(12, 2018, 0.305, 0.392, 155.0, 1.32, 4.15, 8.7)\n",
      "(29, 2018, 0.312, 0.427, 217.0, 1.2, 3.77, 9.5)\n",
      "(14, 2018, 0.313, 0.413, 214.0, 1.25, 4.04, 8.8)\n",
      "(10, 2018, 0.3, 0.38, 135.0, 1.31, 3.65, 8.1)\n",
      "(5, 2018, 0.302, 0.401, 182.0, 1.25, 3.73, 8.9)\n",
      "(28, 2018, 0.318, 0.404, 194.0, 1.24, 3.78, 10.1)\n",
      "(3, 2018, 0.298, 0.391, 188.0, 1.25, 3.75, 9.6)\n",
      "(24, 2018, 0.3, 0.368, 133.0, 1.21, 3.82, 7.6)\n",
      "(15, 2018, 0.303, 0.357, 128.0, 1.15, 3.4, 9.5)\n",
      "(23, 2018, 0.297, 0.38, 162.0, 1.24, 3.73, 8.8)\n",
      "(1, 2018, 0.31, 0.397, 176.0, 1.2, 3.75, 8.8)\n",
      "(19, 2018, 0.312, 0.389, 170.0, 1.28, 3.75, 8.8)\n",
      "(21, 2018, 0.314, 0.393, 186.0, 1.1, 3.11, 10.4)\n",
      "(11, 2017, 0.352, 0.495, 288.0, 1.49, 5.56, 7.9)\n",
      "(17, 2017, 0.338, 0.494, 307.0, 1.46, 5.24, 8.6)\n",
      "(4, 2017, 0.34, 0.466, 245.0, 1.46, 5.6, 7.8)\n",
      "(18, 2017, 0.339, 0.49, 306.0, 1.48, 5.2, 7.8)\n",
      "(30, 2017, 0.342, 0.454, 231.0, 1.46, 5.06, 8.6)\n",
      "(9, 2017, 0.326, 0.456, 224.0, 1.45, 5.18, 9.0)\n",
      "(22, 2017, 0.321, 0.42, 163.0, 1.38, 4.99, 7.7)\n",
      "(6, 2017, 0.314, 0.414, 182.0, 1.3, 4.18, 9.0)\n",
      "(2, 2017, 0.336, 0.452, 249.0, 1.37, 4.53, 8.6)\n",
      "(19, 2017, 0.328, 0.442, 242.0, 1.43, 4.79, 8.3)\n",
      "(13, 2017, 0.338, 0.472, 279.0, 1.43, 4.9, 8.4)\n",
      "(27, 2017, 0.325, 0.431, 217.0, 1.38, 4.7, 10.0)\n",
      "(1, 2017, 0.323, 0.434, 220.0, 1.36, 4.19, 8.6)\n",
      "(5, 2017, 0.331, 0.452, 256.0, 1.38, 5.12, 8.8)\n",
      "(8, 2017, 0.323, 0.432, 223.0, 1.3, 4.24, 9.4)\n",
      "(20, 2017, 0.327, 0.448, 257.0, 1.31, 4.25, 8.8)\n",
      "(28, 2017, 0.319, 0.431, 223.0, 1.3, 4.38, 8.4)\n",
      "(12, 2017, 0.309, 0.401, 162.0, 1.3, 4.6, 9.3)\n",
      "(14, 2017, 0.324, 0.422, 220.0, 1.32, 4.1, 9.0)\n",
      "(3, 2017, 0.31, 0.415, 213.0, 1.3, 4.31, 9.6)\n",
      "(21, 2017, 0.319, 0.427, 215.0, 1.33, 4.4, 9.2)\n",
      "(16, 2017, 0.329, 0.438, 250.0, 1.24, 3.97, 8.0)\n",
      "(26, 2017, 0.322, 0.415, 210.0, 1.29, 4.27, 9.5)\n",
      "(7, 2017, 0.315, 0.422, 227.0, 1.35, 4.74, 8.6)\n",
      "(15, 2017, 0.298, 0.375, 146.0, 1.22, 3.76, 9.4)\n",
      "(10, 2017, 0.294, 0.388, 149.0, 1.27, 3.8, 8.7)\n",
      "(24, 2017, 0.302, 0.392, 167.0, 1.17, 3.65, 9.9)\n",
      "(23, 2017, 0.308, 0.41, 219.0, 1.26, 4.18, 9.7)\n",
      "(25, 2017, 0.316, 0.424, 239.0, 1.13, 3.66, 10.3)\n",
      "(29, 2017, 0.305, 0.428, 247.0, 1.1, 3.37, 9.5)\n",
      "(11, 2016, 0.352, 0.495, 288.0, 1.49, 5.56, 7.9)\n",
      "(17, 2016, 0.338, 0.494, 307.0, 1.46, 5.24, 8.6)\n",
      "(4, 2016, 0.34, 0.466, 245.0, 1.46, 5.6, 7.8)\n",
      "(18, 2016, 0.339, 0.49, 306.0, 1.48, 5.2, 7.8)\n",
      "(30, 2016, 0.342, 0.454, 231.0, 1.46, 5.06, 8.6)\n",
      "(9, 2016, 0.326, 0.456, 224.0, 1.45, 5.18, 9.0)\n",
      "(22, 2016, 0.321, 0.42, 163.0, 1.38, 4.99, 7.7)\n",
      "(6, 2016, 0.314, 0.414, 182.0, 1.3, 4.18, 9.0)\n",
      "(2, 2016, 0.336, 0.452, 249.0, 1.37, 4.53, 8.6)\n",
      "(19, 2016, 0.328, 0.442, 242.0, 1.43, 4.79, 8.3)\n",
      "(13, 2016, 0.338, 0.472, 279.0, 1.43, 4.9, 8.4)\n",
      "(27, 2016, 0.325, 0.431, 217.0, 1.38, 4.7, 10.0)\n",
      "(1, 2016, 0.323, 0.434, 220.0, 1.36, 4.19, 8.6)\n",
      "(5, 2016, 0.331, 0.452, 256.0, 1.38, 5.12, 8.8)\n",
      "(8, 2016, 0.323, 0.432, 223.0, 1.3, 4.24, 9.4)\n",
      "(20, 2016, 0.327, 0.448, 257.0, 1.31, 4.25, 8.8)\n",
      "(28, 2016, 0.319, 0.431, 223.0, 1.3, 4.38, 8.4)\n",
      "(12, 2016, 0.309, 0.401, 162.0, 1.3, 4.6, 9.3)\n",
      "(14, 2016, 0.324, 0.422, 220.0, 1.32, 4.1, 9.0)\n",
      "(3, 2016, 0.31, 0.415, 213.0, 1.3, 4.31, 9.6)\n",
      "(21, 2016, 0.319, 0.427, 215.0, 1.33, 4.4, 9.2)\n",
      "(16, 2016, 0.329, 0.438, 250.0, 1.24, 3.97, 8.0)\n",
      "(26, 2016, 0.322, 0.415, 210.0, 1.29, 4.27, 9.5)\n",
      "(7, 2016, 0.315, 0.422, 227.0, 1.35, 4.74, 8.6)\n",
      "(15, 2016, 0.298, 0.375, 146.0, 1.22, 3.76, 9.4)\n",
      "(10, 2016, 0.294, 0.388, 149.0, 1.27, 3.8, 8.7)\n",
      "(24, 2016, 0.302, 0.392, 167.0, 1.17, 3.65, 9.9)\n",
      "(23, 2016, 0.308, 0.41, 219.0, 1.26, 4.18, 9.7)\n",
      "(25, 2016, 0.316, 0.424, 239.0, 1.13, 3.66, 10.3)\n",
      "(29, 2016, 0.305, 0.428, 247.0, 1.1, 3.37, 9.5)\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT * FROM Stats')\n",
    "results = cur.fetchall()\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Extract data from the API. This is the player data to get total numbers of the final 40 man rosters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2019, 120, 117], [2019, 117, 147], [2019, 120, 138], [2019, 117, 139], [2019, 147, 142], [2019, 120, 119], [2019, 138, 130], [2019, 139, 133], [2019, 120, 158], [2018, 111, 119], [2018, 111, 117], [2018, 119, 158], [2018, 111, 147], [2018, 117, 114], [2018, 158, 115], [2018, 119, 130], [2018, 147, 133], [2018, 115, 112], [2017, 117, 119], [2017, 117, 147], [2017, 119, 112], [2017, 147, 114], [2017, 117, 111], [2017, 119, 109], [2017, 112, 120], [2017, 147, 142], [2017, 109, 115], [2016, 112, 114], [2016, 114, 141], [2016, 112, 119]]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT winning_team, losing_team, year FROM Series')\n",
    "results = cur.fetchall()\n",
    "\n",
    "year_and_win = []\n",
    "year_and_lose = []\n",
    "year_and_win_and_lose = []\n",
    "for row in results:\n",
    "    year_and_win.append([row[2],MLB_functions.reading(row[0])])\n",
    "    year_and_lose.append([row[2],MLB_functions.reading(row[1])])\n",
    "    \n",
    "    year_and_win_and_lose.append([row[2],MLB_functions.reading(row[0]),MLB_functions.reading(row[1])])\n",
    "\n",
    "print(year_and_win_and_lose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are needed to scrape the data in the final cell of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playerID(response_roster):\n",
    "    \"\"\"\n",
    "    The following function takes one roster from the MLB API roster_retrieval function as an input\n",
    "    outputs an array of the player ID's for the input team\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    import re\n",
    "    global player_id_array\n",
    "    player_id_string_array = [m.start() for m in re.finditer('player_id', response_roster.text)]\n",
    "    player_id_array = []\n",
    "    for val in player_id_string_array:\n",
    "        val = int(val)\n",
    "        player_id = response_roster.text[val+12:val+18]\n",
    "        player_id_array.append(player_id)\n",
    "    #     print(player_id)\n",
    "    return player_id_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roster_retrieval(year,team):\n",
    "    \"\"\"\n",
    "    The function uses the year and the team ID input to scrape a roster from the MLB API\n",
    "    The function outputs the roster information\n",
    "    Use the year_and_win_and_lose array data\n",
    "    \"\"\"\n",
    "    \n",
    "    response_roster = []\n",
    "    url = \"https://mlb-data.p.rapidapi.com/json/named.roster_team_alltime.bam\"\n",
    "\n",
    "    querystring = {\"all_star_sw\":\"'N'\",\"sort_order\":\"name_asc\",\"end_season\":str(year),\"team_id\":str(team),\"start_season\":str(year-1)}\n",
    "\n",
    "    headers = {\n",
    "        'x-rapidapi-host': \"mlb-data.p.rapidapi.com\",\n",
    "        'x-rapidapi-key': \"52d72158damshf14c292f35c4bdap1ec354jsnc179a7e5ec57\"\n",
    "        }\n",
    "    response_roster.append(requests.request(\"GET\", url, headers=headers, params=querystring)) #response_roster\n",
    "    return response_roster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_player_name(val):\n",
    "    \"\"\"\n",
    "    The function uses a player ID as an input\n",
    "    Outputs the name of the player associated with that ID\n",
    "    Iterate through the output of the player_ID function\n",
    "    \"\"\"   \n",
    "    \n",
    "    url_2 = \"https://mlb-data.p.rapidapi.com/json/named.player_info.bam\"\n",
    "    querystring_2 = {\"sport_code\":\"'mlb'\",\"player_id\":str(val)}\n",
    "    \n",
    "    headers = {\n",
    "    'x-rapidapi-host': \"mlb-data.p.rapidapi.com\",\n",
    "    'x-rapidapi-key': \"52d72158damshf14c292f35c4bdap1ec354jsnc179a7e5ec57\"\n",
    "    }\n",
    "    \n",
    "    response_2 = requests.request(\"GET\", url_2, headers=headers, params=querystring_2)\n",
    "    return response_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plaer_stat(val):\n",
    "    \"\"\"\n",
    "    The function uses the player ID as an input\n",
    "    Outputs the statistical profile for that player\n",
    "    Iterate through the output of the player_ID function\n",
    "    \"\"\"\n",
    "    url = \"https://mlb-data.p.rapidapi.com/json/named.sport_hitting_tm.bam\"\n",
    "    querystring = {\"season\":str(year),\"player_id\":str(val),\"league_list_id\":\"'mlb'\",\"game_type\":\"'R'\"}\n",
    "    \n",
    "    headers = {\n",
    "    'x-rapidapi-host': \"mlb-data.p.rapidapi.com\",\n",
    "    'x-rapidapi-key': \"52d72158damshf14c292f35c4bdap1ec354jsnc179a7e5ec57\"\n",
    "    }\n",
    "    \n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the API for the data.  The desired data is the total number of RBIs collects by the entire final roster of each team. The Idea for including this is to account for mid season additions. Remember that teams can trade for players in the middile of the season which means the team stats at the end of the year might not accuratly represent the team entering the playoffs. This will also explain the higher total values extracted.\n",
    "<br>\n",
    "<br>\n",
    "The way this works is that I need to create an array containing each team for each series called out above.  I then need to scrape the API for the final roster of each playoff team. After that I needed to get the player_ID for each team on that roster. Subsequently I had to get the statistical profile for each player ID. Then the scraper parses the profile for the desired statistics. For this I focused on total team RBI accumulated by each player on the final roster.\n",
    "<br>\n",
    "<br>\n",
    "WARNING: The below code requires a lot of processing and could take a while to run. (It took me 21 min for a sample of 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of RBI in 2019 for win team 120 is 1373\n",
      "the total number of RBI in 2019 for lose team 117 is 1384\n",
      "the total number of RBI in 2019 for win team 117 is 1380\n",
      "the total number of RBI in 2019 for lose team 147 is 1619\n",
      "the total number of RBI in 2019 for win team 120 is 1373\n",
      "the total number of RBI in 2019 for lose team 138 is 1231\n",
      "the total number of RBI in 2019 for win team 117 is 1416\n",
      "the total number of RBI in 2019 for lose team 139 is 1455\n",
      "the total number of RBI in 2019 for win team 147 is 1628\n",
      "the total number of RBI in 2019 for lose team 142 is 1750\n",
      "the total number of RBI in 2019 for win team 120 is 1373\n",
      "the total number of RBI in 2019 for lose team 119 is 1600\n",
      "the total number of RBI in 2019 for win team 138 is 1231\n",
      "the total number of RBI in 2019 for lose team 130 is 0\n",
      "the total number of RBI in 2019 for win team 139 is 1455\n",
      "the total number of RBI in 2019 for lose team 133 is 1418\n",
      "the total number of RBI in 2019 for win team 120 is 1373\n",
      "the total number of RBI in 2019 for lose team 158 is 1417\n",
      "the total number of RBI in 2018 for win team 111 is 1101\n",
      "the total number of RBI in 2018 for lose team 119 is 1257\n",
      "the total number of RBI in 2018 for win team 111 is 1004\n",
      "the total number of RBI in 2018 for lose team 117 is 1359\n",
      "the total number of RBI in 2018 for win team 119 is 1257\n",
      "the total number of RBI in 2018 for lose team 158 is 1328\n",
      "the total number of RBI in 2018 for win team 111 is 1004\n",
      "the total number of RBI in 2018 for lose team 147 is 1448\n",
      "the total number of RBI in 2018 for win team 117 is 1359\n",
      "the total number of RBI in 2018 for lose team 114 is 1166\n",
      "the total number of RBI in 2018 for win team 158 is 1328\n",
      "the total number of RBI in 2018 for lose team 115 is 974\n",
      "the total number of RBI in 2018 for win team 119 is 1257\n",
      "the total number of RBI in 2018 for lose team 130 is 0\n",
      "the total number of RBI in 2018 for win team 147 is 1448\n",
      "the total number of RBI in 2018 for lose team 133 is 1294\n",
      "the total number of RBI in 2018 for win team 115 is 1035\n",
      "the total number of RBI in 2018 for lose team 112 is 962\n",
      "the total number of RBI in 2017 for win team 117 is 1190\n",
      "the total number of RBI in 2017 for lose team 119 is 953\n",
      "the total number of RBI in 2017 for win team 117 is 1190\n",
      "the total number of RBI in 2017 for lose team 147 is 1441\n",
      "the total number of RBI in 2017 for win team 119 is 953\n",
      "the total number of RBI in 2017 for lose team 112 is 997\n",
      "the total number of RBI in 2017 for win team 147 is 1481\n",
      "the total number of RBI in 2017 for lose team 114 is 1334\n",
      "the total number of RBI in 2017 for win team 117 is 1190\n",
      "the total number of RBI in 2017 for lose team 111 is 1410\n",
      "the total number of RBI in 2017 for win team 119 is 953\n",
      "the total number of RBI in 2017 for lose team 109 is 1251\n",
      "the total number of RBI in 2017 for win team 112 is 997\n",
      "the total number of RBI in 2017 for lose team 120 is 1043\n",
      "the total number of RBI in 2017 for win team 147 is 1539\n",
      "the total number of RBI in 2017 for lose team 142 is 1484\n",
      "the total number of RBI in 2017 for win team 109 is 1251\n",
      "the total number of RBI in 2017 for lose team 115 is 1224\n",
      "the total number of RBI in 2016 for win team 112 is 1385\n",
      "the total number of RBI in 2016 for lose team 114 is 1380\n",
      "the total number of RBI in 2016 for win team 114 is 1304\n",
      "the total number of RBI in 2016 for lose team 141 is 1159\n",
      "the total number of RBI in 2016 for win team 112 is 1385\n",
      "the total number of RBI in 2016 for lose team 119 is 948\n",
      "1285.7810451984406\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "team_total = []\n",
    "val_rbi = []\n",
    "# year_and_win_and_lose =[[2019, 120, 117], [2019, 117, 147], [2019, 120, 138], [2019, 117, 139], [2019, 147, 142], [2019, 120, 119], [2019, 138, 130], [2019, 139, 133], [2019, 120, 158], [2018, 111, 119]]\n",
    "for row in year_and_win_and_lose:\n",
    "    year = row[0]\n",
    "    winning_tm = row[1]\n",
    "    losing_tm = row[2]\n",
    "    winning_roster = roster_retrieval(year,winning_tm) # Retreive the rosters\n",
    "    losing_roster = roster_retrieval(year,losing_tm)\n",
    "    player_id_winning = playerID(winning_roster[0]) # Retreive the player Id for each player on the roster\n",
    "    player_id_losing = playerID(losing_roster[0])\n",
    "    \n",
    "    team_total_RBI_win = []\n",
    "    \n",
    "    for val in player_id_winning:\n",
    "        \n",
    "#         player_name = get_player_name(val)   # Get the name of each player\n",
    "        player_stat = get_plaer_stat(val) # Get the statistical profile for each player ID\n",
    "\n",
    "        leng = len(player_stat.text.split('rbi'))\n",
    "        try:\n",
    "            RBI = player_stat.text.split(\"rbi\")[1].split(',')[0].split(\":\")[1][1:-1]  # parse the data for RBI total\n",
    "        except:\n",
    "            pass\n",
    "        else:\n",
    "            RBI_array = []\n",
    "            i = len(player_stat.text.split(\"rbi\"))\n",
    "            while i > 1:\n",
    "                RBI = player_stat.text.split(\"rbi\")[i-1].split(',')[0].split(\":\")[1][1:-1] # Some players were on more than one team\n",
    "                i = i - 1\n",
    "                RBI_array.append(int(RBI))\n",
    "\n",
    "        team_total_RBI_win.append(sum(RBI_array)) #Sum the total RBI for each player on the final roster\n",
    "        \n",
    "    print(f\"the total number of RBI in {year} for win team {winning_tm} is {sum(team_total_RBI_win)}\")\n",
    "\n",
    "    team_total_RBI_lose = []\n",
    "    \n",
    "    for val in player_id_losing:\n",
    "        \n",
    "#         player_name = get_player_name(val)   \n",
    "        player_stat = get_plaer_stat(val)\n",
    "\n",
    "        leng = len(player_stat.text.split('rbi'))\n",
    "        \n",
    "        try:\n",
    "            RBI = player_stat.text.split(\"rbi\")[1].split(',')[0].split(\":\")[1][1:-1] # Repeat for losing team in series\n",
    "        except IndexError:\n",
    "            pass\n",
    "        else:\n",
    "            RBI_array = []\n",
    "            i = len(player_stat.text.split(\"rbi\"))\n",
    "            while i > 1:\n",
    "                RBI = player_stat.text.split(\"rbi\")[i-1].split(',')[0].split(\":\")[1][1:-1]\n",
    "                i = i - 1\n",
    "                RBI_array.append(int(RBI))\n",
    "\n",
    "        team_total_RBI_lose.append(sum(RBI_array))\n",
    "        \n",
    "    print(f\"the total number of RBI in {year} for lose team {losing_tm} is {sum(team_total_RBI_lose)}\")\n",
    "\n",
    "    val_rbi.append((int(year),int(winning_tm),int(sum(team_total_RBI_win)),int(losing_tm),int(sum(team_total_RBI_lose))))\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: Analysis 1 of 2\n",
    "##### First analysis will be simply looking at a teams statistics and running a logistic regression using if they won or lost a series as the target. This mean the team that wins the world series will be assigned a target value of 1 for every series that they played. The team that lost will be assigned a target value of 1 for every series they played except the last. This continues for every team in the playoffs. The classifier is now simply \"win\" or \"not win (lose)\". First Analysis does not incoorporate API data. This makes it ideal for trouble shooting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(30, 2019), (11, 2019), (30, 2019), (11, 2019), (19, 2019), (30, 2019), (26, 2019), (27, 2019), (30, 2019), (4, 2018), (4, 2018), (14, 2018), (4, 2018), (11, 2018), (16, 2018), (14, 2018), (19, 2018), (9, 2018), (11, 2017), (11, 2017), (14, 2017), (19, 2017), (11, 2017), (14, 2017), (5, 2017), (19, 2017), (1, 2017), (5, 2016), (8, 2016), (5, 2016)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT * FROM Series')\n",
    "#  series, winning, losing, year\n",
    "results = cur.fetchall()\n",
    "win = []\n",
    "lose = []\n",
    "for row in results:\n",
    "    win.append((row[1],row[3]))\n",
    "    lose.append((row[2],row[3]))\n",
    "print(win)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_retreival(winner, loser):\n",
    "    try:\n",
    "        cmd_line = \"SELECT * FROM Stats WHERE team_id = %d and year = %d\" % (winner)\n",
    "        cur.execute(cmd_line)\n",
    "        results_winner = cur.fetchall()\n",
    "    except: \n",
    "        results_winner = 'NaN'\n",
    "    \n",
    "    try:\n",
    "        cmd_line = \"SELECT * FROM Stats WHERE team_id = %d and year = %d\" % (loser)\n",
    "        cur.execute(cmd_line)\n",
    "        results_loser = cur.fetchall()\n",
    "    except:\n",
    "        results_loser = 'NaN'\n",
    "        \n",
    "   \n",
    "    return (results_winner, results_loser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# team_id, year, OBP, SLG, HR, WHIP, ERA, SO9\n",
    "# Changed RBI to HR\n",
    "logistic_reg_array = []\n",
    "for i in range(len(win)):\n",
    "#     stat_retreival(win[i], lose[i])\n",
    "    (w,l) = stat_retreival(win[i], lose[i])\n",
    "    logistic_reg_array.append(w)\n",
    "    logistic_reg_array.append(l)\n",
    "#     logistic_reg_array.append(win[i], lose[i])\n",
    "# print(logistic_reg_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# team_id, year, OBP, SLG, HR, WHIP, ERA, SO9\n",
    "blank = []\n",
    "counter = 0\n",
    "for row in logistic_reg_array:\n",
    "    if row == 'NaN':\n",
    "        blank.append([np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,])\n",
    "    else:\n",
    "        if counter % 2 == 0:\n",
    "            blank.append([row[0][0],row[0][1],row[0][2],row[0][3],row[0][4],row[0][5],row[0][6],row[0][7], 1])\n",
    "        else:\n",
    "            blank.append([row[0][0],row[0][1],row[0][2],row[0][3],row[0][4],row[0][5],row[0][6],row[0][7], 0])\n",
    "    counter += 1\n",
    "#     print(counter)\n",
    "\n",
    "# put this into a function that takes the training vec size as an input and returns the test and training vec\n",
    "training_vec = blank[0:40]\n",
    "test_vec = []\n",
    "for val in blank:\n",
    "    if val not in training_vec:\n",
    "        test_vec.append(val)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the data into a dataframe offers two key advantages: 1) it allows us to clearly see and manipulate the data. 2) It is very compatable with performing the logistic regression analysis. (Oh and it demonstrates knowledge with pandas!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>year</th>\n",
       "      <th>team OBP</th>\n",
       "      <th>team SLG</th>\n",
       "      <th>team HR</th>\n",
       "      <th>team WHIP</th>\n",
       "      <th>team ERA</th>\n",
       "      <th>team SO9</th>\n",
       "      <th>winLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.454</td>\n",
       "      <td>231.0</td>\n",
       "      <td>1.46</td>\n",
       "      <td>5.06</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.495</td>\n",
       "      <td>288.0</td>\n",
       "      <td>1.49</td>\n",
       "      <td>5.56</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.495</td>\n",
       "      <td>288.0</td>\n",
       "      <td>1.49</td>\n",
       "      <td>5.56</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.442</td>\n",
       "      <td>242.0</td>\n",
       "      <td>1.43</td>\n",
       "      <td>4.79</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.454</td>\n",
       "      <td>231.0</td>\n",
       "      <td>1.46</td>\n",
       "      <td>5.06</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   team    year  team OBP  team SLG  team HR  team WHIP  team ERA  team SO9  \\\n",
       "0  30.0  2019.0     0.342     0.454    231.0       1.46      5.06       8.6   \n",
       "1  11.0  2019.0     0.352     0.495    288.0       1.49      5.56       7.9   \n",
       "2  11.0  2019.0     0.352     0.495    288.0       1.49      5.56       7.9   \n",
       "3  19.0  2019.0     0.328     0.442    242.0       1.43      4.79       8.3   \n",
       "4  30.0  2019.0     0.342     0.454    231.0       1.46      5.06       8.6   \n",
       "\n",
       "   winLoss  \n",
       "0      1.0  \n",
       "1      0.0  \n",
       "2      1.0  \n",
       "3      0.0  \n",
       "4      1.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# team_id, year, OBP, SLG, HR, WHIP, ERA, SO9\n",
    "\n",
    "df = pd.DataFrame(np.array(blank), columns=['team', 'year','team OBP','team SLG', 'team HR','team WHIP',\n",
    "                               'team ERA','team SO9','winLoss'])\n",
    "\n",
    "df = df.dropna(thresh=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log:  0.6666666666666666\n",
      "[[-0.02449455 -0.02279686 -0.00822853 -0.07313834  0.81760863 -0.16154747]]\n",
      "Logist \r",
      " [[4 4]\n",
      " [0 4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peter Argo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame(np.array(blank), columns=['year', 'team', 'team OBP','team SLG', 'team HR','team WHIP',\n",
    "#                                'team ERA','team SO9','win Loss'])\n",
    "# # https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python\n",
    "x_data = df[['team OBP','team SLG', 'team HR','team WHIP','team ERA','team SO9']]\n",
    "y_data = df.winLoss\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2, random_state = 12)\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "prediction = dict()\n",
    "prediction['Logistic'] = model.predict(x_test)\n",
    "print('Log: ',accuracy_score(y_test, prediction['Logistic']))\n",
    "print(model.coef_)\n",
    "conf_mat_logist = confusion_matrix(y_test, prediction['Logistic'])\n",
    "print('Logist \\r', conf_mat_logist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: Using inductive bias I assigned a test size of 20% of my data set. Using this, method of analysis, we see an improvement on our intial 50% baseline. Recall that using the 190 maximum sample size showed that approximatly 50% of the time does the team with the better record win the series. Using the features clearly shown in the pandas table, we can predict games at approximatly a 61% accuracy. Pretty Good! However, we completely recognize this is statistically fragile as the test and training set are not as comprhensive as ideal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5: Analysis 2 of 2\n",
    "##### In an attempt to improve on the results, we readjusted out method of analysis. The initial goal was to use home team performance as our target, however the data sources did not lend itself kindly to retreiving this data. Therefore, I wanted to analyize performance of team with a better record. Our classifier is thus not \"Winning team had better record\" vs. \"Winning Team had worse record\" (assigned value of 1 and 0 respectively). For simplistic sake, if the winning team had the same record as the losing team, that instance would be assigned a value of 0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0]\n",
      "[(30, 2019), (11, 2019), (30, 2019), (11, 2019), (19, 2019), (30, 2019), (26, 2019), (27, 2019), (30, 2019), (4, 2018), (4, 2018), (14, 2018), (4, 2018), (11, 2018), (16, 2018), (14, 2018), (19, 2018), (9, 2018), (11, 2017), (11, 2017), (14, 2017), (19, 2017), (11, 2017), (14, 2017), (5, 2017), (19, 2017), (1, 2017), (5, 2016), (8, 2016), (5, 2016)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT * FROM Series')\n",
    "#  series, winning, losing, year\n",
    "results = cur.fetchall()\n",
    "win = []\n",
    "lose = []\n",
    "target = []\n",
    "for row in results:\n",
    "    win.append((row[1],row[3]))\n",
    "    lose.append((row[2],row[3]))\n",
    "    target.append(row[4])\n",
    "print(target)\n",
    "print(win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_retreival(winner, loser):\n",
    "    try:\n",
    "        cmd_line = \"SELECT * FROM Stats WHERE team_id = %d and year = %d\" % (winner)\n",
    "        cur.execute(cmd_line)\n",
    "        results_winner = cur.fetchall()\n",
    "    except: \n",
    "        results_winner = 'NaN'\n",
    "    \n",
    "    try:\n",
    "        cmd_line = \"SELECT * FROM Stats WHERE team_id = %d and year = %d\" % (loser)\n",
    "        cur.execute(cmd_line)\n",
    "        results_loser = cur.fetchall()\n",
    "    except:\n",
    "        results_loser = 'NaN'\n",
    "        \n",
    "   \n",
    "    return (results_winner, results_loser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg_array2 = []\n",
    "for i in range(len(win)):\n",
    "    logistic_reg_array2.append([stat_retreival(win[i], lose[i]),target[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells combines our API data with our fos sports team stat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2019, 30, 1373, 11, 1384], [2019, 11, 1380, 19, 1619], [2019, 30, 1373, 26, 1231], [2019, 11, 1416, 27, 1455], [2019, 19, 1628, 17, 1750], [2019, 30, 1373, 14, 1600], [2019, 26, 1231, 2, 0], [2019, 27, 1455, 20, 1418], [2019, 30, 1373, 16, 1417], [2018, 4, 1101, 14, 1257], [2018, 4, 1004, 11, 1359], [2018, 14, 1257, 16, 1328], [2018, 4, 1004, 19, 1448], [2018, 11, 1359, 8, 1166], [2018, 16, 1328, 9, 974], [2018, 14, 1257, 2, 0], [2018, 19, 1448, 20, 1294], [2018, 9, 1035, 5, 962], [2017, 11, 1190, 14, 953], [2017, 11, 1190, 19, 1441], [2017, 14, 953, 5, 997], [2017, 19, 1481, 8, 1334], [2017, 11, 1190, 4, 1410], [2017, 14, 953, 1, 1251], [2017, 5, 997, 30, 1043], [2017, 19, 1539, 17, 1484], [2017, 1, 1251, 9, 1224], [2016, 5, 1385, 8, 1380], [2016, 8, 1304, 29, 1159], [2016, 5, 1385, 14, 948]]\n"
     ]
    }
   ],
   "source": [
    "new_val_rbi = []\n",
    "for row in val_rbi:\n",
    "    cmd_line = \"SELECT team_id FROM prim_ID WHERE API_ID = %d\" % row[1]\n",
    "    cur.execute(cmd_line)\n",
    "    results_winner_flip = cur.fetchall()\n",
    "    \n",
    "    cmd_line = \"SELECT team_id FROM prim_ID WHERE API_ID = %d\" % row[3]\n",
    "    cur.execute(cmd_line)\n",
    "    results_loser_flip = cur.fetchall()\n",
    "    new_val_rbi.append([row[0],results_winner_flip[0][0],row[2],results_loser_flip[0][0],row[4]])\n",
    "\n",
    "print(new_val_rbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg_array3 = []\n",
    "for i in range(len(new_val_rbi)):\n",
    "    if new_val_rbi[i][1] == logistic_reg_array2[i][0][0][0][0] and new_val_rbi[i][3] == logistic_reg_array2[i][0][1][0][0]:\n",
    "        logistic_reg_array3.append([logistic_reg_array2[i],new_val_rbi[i][2],new_val_rbi[i][4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank2 = []\n",
    "counter = 0\n",
    "for row in logistic_reg_array3:\n",
    "#     print(row[0][1][0][2])\n",
    "    try:\n",
    "        blank2.append([row[0][0][0][0][1],row[0][0][0][0][0],row[0][0][0][0][2],row[0][0][0][0][3],row[0][0][0][0][4],\\\n",
    "                   row[0][0][0][0][5],row[0][0][0][0][6],row[0][0][0][0][7],row[1],row[0][0][1][0][0],row[0][0][1][0][2],\\\n",
    "                   row[0][0][1][0][3],row[0][0][1][0][4],row[0][0][1][0][5],row[0][0][1][0][6],row[0][0][1][0][7],row[2],\\\n",
    "                   row[0][1]])\n",
    "    except:\n",
    "        blank2.append([np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan])\n",
    "    counter += 1\n",
    "#     print(counter)\n",
    "\n",
    "# print(blank2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>winning team</th>\n",
       "      <th>winning team OBP</th>\n",
       "      <th>winning team SLG</th>\n",
       "      <th>winning team HR</th>\n",
       "      <th>winning team WHIP</th>\n",
       "      <th>winning team ERA</th>\n",
       "      <th>winning team SO9</th>\n",
       "      <th>winning team rbi</th>\n",
       "      <th>losing team</th>\n",
       "      <th>...</th>\n",
       "      <th>losing team SO9</th>\n",
       "      <th>losing team rbi</th>\n",
       "      <th>Target</th>\n",
       "      <th>diff OBP</th>\n",
       "      <th>diff SLG</th>\n",
       "      <th>diff HR</th>\n",
       "      <th>diff WHIP</th>\n",
       "      <th>diff ERA</th>\n",
       "      <th>diff SO9</th>\n",
       "      <th>diff RBI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.454</td>\n",
       "      <td>231.0</td>\n",
       "      <td>1.46</td>\n",
       "      <td>5.06</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1384.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-57.0</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.495</td>\n",
       "      <td>288.0</td>\n",
       "      <td>1.49</td>\n",
       "      <td>5.56</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1380.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.053</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.77</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.454</td>\n",
       "      <td>231.0</td>\n",
       "      <td>1.46</td>\n",
       "      <td>5.06</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.5</td>\n",
       "      <td>1231.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.039</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.79</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.495</td>\n",
       "      <td>288.0</td>\n",
       "      <td>1.49</td>\n",
       "      <td>5.56</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1416.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1455.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.064</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>-39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.442</td>\n",
       "      <td>242.0</td>\n",
       "      <td>1.43</td>\n",
       "      <td>4.79</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1628.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1750.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-122.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  winning team  winning team OBP  winning team SLG  winning team HR  \\\n",
       "0  2019.0          30.0             0.342             0.454            231.0   \n",
       "1  2019.0          11.0             0.352             0.495            288.0   \n",
       "2  2019.0          30.0             0.342             0.454            231.0   \n",
       "3  2019.0          11.0             0.352             0.495            288.0   \n",
       "4  2019.0          19.0             0.328             0.442            242.0   \n",
       "\n",
       "   winning team WHIP  winning team ERA  winning team SO9  winning team rbi  \\\n",
       "0               1.46              5.06               8.6            1373.0   \n",
       "1               1.49              5.56               7.9            1380.0   \n",
       "2               1.46              5.06               8.6            1373.0   \n",
       "3               1.49              5.56               7.9            1416.0   \n",
       "4               1.43              4.79               8.3            1628.0   \n",
       "\n",
       "   losing team  ...  losing team SO9  losing team rbi  Target  diff OBP  \\\n",
       "0         11.0  ...              7.9           1384.0     1.0    -0.010   \n",
       "1         19.0  ...              8.3           1619.0     1.0     0.024   \n",
       "2         26.0  ...              9.5           1231.0     1.0     0.020   \n",
       "3         27.0  ...             10.0           1455.0     0.0     0.027   \n",
       "4         17.0  ...              8.6           1750.0     1.0    -0.010   \n",
       "\n",
       "   diff SLG  diff HR  diff WHIP  diff ERA  diff SO9  diff RBI  \n",
       "0    -0.041    -57.0      -0.03     -0.50       0.7     -11.0  \n",
       "1     0.053     46.0       0.06      0.77      -0.4    -239.0  \n",
       "2     0.039     21.0       0.17      0.79      -0.9     142.0  \n",
       "3     0.064     71.0       0.11      0.86      -2.1     -39.0  \n",
       "4    -0.052    -65.0      -0.03     -0.45      -0.3    -122.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(np.array(blank2), columns=['year', 'winning team', 'winning team OBP','winning team SLG', 'winning team HR','winning team WHIP',\n",
    "                               'winning team ERA','winning team SO9','winning team rbi','losing team', 'losing team OBP','losing team SLG', 'losing team HR','losing team WHIP',\n",
    "                               'losing team ERA','losing team SO9','losing team rbi','Target'])\n",
    "df2['diff OBP'] = df2['winning team OBP'] - df2['losing team OBP']\n",
    "df2['diff SLG'] = df2['winning team SLG'] - df2['losing team SLG']\n",
    "df2['diff HR'] = df2['winning team HR'] - df2['losing team HR']\n",
    "df2['diff WHIP'] = df2['winning team WHIP'] - df2['losing team WHIP']\n",
    "df2['diff ERA'] = df2['winning team ERA'] - df2['losing team ERA']\n",
    "df2['diff SO9'] = df2['winning team SO9'] - df2['losing team SO9']\n",
    "df2['diff RBI'] = df2['winning team rbi'] - df2['losing team rbi']\n",
    "df2 = df2.dropna(thresh=1)\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model is accurate to:  0.4666666666666667\n",
      "[[ 0.02543677  0.03107281 -0.04735554  0.19393876  0.20225825 -1.13632932\n",
      "  -0.00573305]]\n",
      "Logist \r",
      " [[3 4]\n",
      " [4 4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peter Argo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "x_data = df2[['diff OBP','diff SLG', 'diff HR','diff WHIP','diff ERA','diff SO9','diff RBI']]\n",
    "y_data = df2.Target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.5, random_state = 12)\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "prediction = dict()\n",
    "prediction['Logistic'] = model.predict(x_test)\n",
    "print('This model is accurate to: ',accuracy_score(y_test, prediction['Logistic']))\n",
    "print(model.coef_)\n",
    "conf_mat_logist = confusion_matrix(y_test, prediction['Logistic'])\n",
    "print('Logist \\r', conf_mat_logist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: I increased the test size partition because we are working with 50 samples as opposed to 190 as in analysis 1.  We see a 53% prediction accuracy however nothing can be concluded with only 50 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
